import jieba
import re
from collections import Counter
import pandas as pd

class DoubanTextCleaner:
    def __init__(self):
        # åˆå§‹åŒ–jiebaåˆ†è¯å™¨ï¼ŒåŠ è½½è‡ªå®šä¹‰è¯å…¸å’Œåœç”¨è¯
        self._setup_jieba()
        
    def _setup_jieba(self):
        """é…ç½®jiebaåˆ†è¯å™¨ï¼ŒåŠ è½½è‡ªå®šä¹‰è¯å…¸å’Œåœç”¨è¯"""
        # åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆå‡è®¾æ–‡ä»¶è·¯å¾„ç›¸åŒï¼‰
        jieba.load_userdict('./user.dict.utf8')
        
        # æ·»åŠ è‡ªå®šä¹‰è¯è¯­
        custom_words = ["é’¢é“ä¾ ", "è¶…çº§è‹±é›„", "è¶…è‹±ç”µå½±", "å¯¡å§", "ç¾é˜Ÿ",
                       "å°èœ˜è››", "æ²»æ„ˆäºº", "é›·éœ†ç‰¹å·¥é˜Ÿ", "æ¨¡ä»¿å¤§å¸ˆ", "å¶è²å¨œ",
                       "ç“¦ä¼¦è’‚å¨œ", "ç¾å›½é˜Ÿé•¿", "ç¾å›½å¯†æ¢", "åè‹±é›„"]
        for word in custom_words:
            jieba.add_word(word)
        
        # åŠ è½½åœç”¨è¯ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰
        try:
            with open('stopwords_hit.txt', 'r', encoding='utf-8') as f:
                self.stopwords = set([line.strip() for line in f])
        except FileNotFoundError:
            self.stopwords = set()
    
    def clean_text(self, text):
        """
        æ–‡æœ¬æ¸…æ´—
        :param text: è¾“å…¥æ–‡æœ¬
        :return: æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not isinstance(text, str):
            return ''
            
        # å®šä¹‰æ‰€æœ‰éœ€è¦æ¸…é™¤çš„æ¨¡å¼
        patterns = [
            r'[[:punct:]]|[\n]',           # æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦
            r'[\U0001F600-\U0001F64F]',    # è¡¨æƒ…ç¬¦å·
            r'[\U0001F300-\U0001F5FF]',    # å…¶ä»–ç¬¦å·
            r'[\U0001F680-\U0001F6FF]',    # äº¤é€šå’Œåœ°å›¾ç¬¦å·
            r'[\U0001F1E0-\U0001F1FF]',    # å›½æ——
            r'[\U00002695-\U0000269F]',    # æ‚é¡¹ç¬¦å·
            r'[\U00002600-\U00002B55]',    # æ›´å¤šç¬¦å·
            r'[\U0000231A-\U0000231B]',    # æ—¶é’Ÿ
            r'[\U000025FB-\U000025FE]',    # å‡ ä½•å›¾å½¢
            r'[\U00002000-\U0000206F]',    # é€šç”¨æ ‡ç‚¹
            r'[\d]',                       # æ•°å­—
            r'[a-zA-Z]',                   # è‹±æ–‡å­—æ¯
            r'\s{2,}',                     # å¤šä¸ªç©ºç™½å­—ç¬¦
            r'[\u200b\u200c\u200d]'        # é›¶å®½å­—ç¬¦
        ]
        
        for pattern in patterns:
            text = re.sub(pattern, '', text)
            
        # å»é™¤æ‰€æœ‰å¤šä½™ç©ºæ ¼ï¼ˆåŒ…æ‹¬è¿ç»­ç©ºæ ¼ï¼‰
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def segment_text(self, text):
        """
        å®Œæ•´çš„åˆ†è¯å¤„ç†æµç¨‹:
        1. æ–‡æœ¬æ¸…æ´—
        2. ä¸­æ–‡åˆ†è¯
        3. å»é™¤åœç”¨è¯
        """
        cleaned_text = self.clean_text(text)
        words = jieba.lcut(cleaned_text)
        # å»é™¤åœç”¨è¯å’Œå•å­—
        words = [word for word in words if word not in self.stopwords and len(word) > 1]
        return words
    
    def preprocess_for_wordcloud(self, texts, type_filter=None):
        """
        ä¸ºè¯äº‘å›¾å‡†å¤‡çš„é¢„å¤„ç†
        :param texts: æ–‡æœ¬åˆ—è¡¨(å¯ä»¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨)
        :param type_filter: å¯é€‰çš„è¯„è®ºç±»å‹è¿‡æ»¤
        :return: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨
        """
        if type_filter and isinstance(texts[0], dict):
            texts = [t for t in texts if t.get('Type') == type_filter]
        texts = pd.DataFrame(texts).drop_duplicates(subset=['Comment'])
        texts = texts.dropna(subset=['Comment'])
        texts = texts['Comment'].tolist()
        all_words = []
        for text in texts:
            if isinstance(text, dict):
                content = text.get('Comment', '')
            else:
                content = str(text)
            words = self.segment_text(content)
            all_words.extend(words)
        return all_words
    
    def preprocess_for_analysis(self, text):
        """
        ä¸ºæƒ…æ„Ÿåˆ†æå’ŒLSTMå‡†å¤‡çš„é¢„å¤„ç†:
        è¿”å›ç©ºæ ¼åˆ†éš”çš„æ¸…æ´—åè¯åºåˆ—
        """
        words = self.segment_text(text)
        return ' '.join(words)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    cleaner = DoubanTextCleaner()
    
    # æµ‹è¯•æ•°æ®
    test_cases = [
        "é’¢é“ä¾ 3ï¸âƒ£æ˜¯2020å¹´æœ€æ£’ğŸ‘çš„è¶…çº§è‹±é›„ç”µå½±ï¼IMDbè¯„åˆ†8.5â­",
        {"Comment": "å¯¡å§ğŸ™„çš„è¡¨ç°æƒŠè‰³ï¼å‰§æƒ…â­â­â­", "Type": "å¥½è¯„"},
        12345,
        "è¿™éƒ¨ç”µå½±å¾ˆä¸€èˆ¬......"
    ]
    
    print("=== æ¸…æ´—æµ‹è¯• ===")
    for case in test_cases:
        if isinstance(case, dict):
            content = case['Comment']
        else:
            content = str(case)
        cleaned = cleaner.clean_text(content)
        print(f"åŸå§‹: {content}\næ¸…æ´å: {cleaned}\n")
    
    print("\n=== åˆ†è¯æµ‹è¯• ===")
    sample = "ç¾å›½é˜Ÿé•¿å’Œé’¢é“ä¾ åœ¨ã€Šå¤ä»‡è€…è”ç›Ÿã€‹ä¸­çš„å¯¹å†³ä»¤äººéš¾å¿˜ï¼"
    print(f"åŸå§‹: {sample}")
    print(f"åˆ†è¯: {cleaner.segment_text(sample)}")
    
    print("\n=== å®Œæ•´é¢„å¤„ç†æµ‹è¯• ===")
    processed = cleaner.preprocess_for_analysis(sample)
    print(f"LSTMè¾“å…¥: {processed}")
